---
title: 2023 honours multivariate FA practical
format:
  pdf:
    embed-resources: true
---

## Question One (4)

Show that the covariance matrix 

$$
\mathbf{\rho} = 
\begin{bmatrix}
1 & 0.63 & 0.45 \\
0.63 & 1 & 0.35 \\
0.45 & 0.35 & 1 
\end{bmatrix}
$$

for the $p=3$ standardised random variables $Z_1$, $Z_2$ and $Z_3$ can be generated by the $m=1$ factor model 

$$
\begin{align*}
Z_1 = 0.9F_1 + \epsilon_1 \\
Z_2 = 0.7F_1 + \epsilon_2 \\
Z_3 = 0.5F_1 + \epsilon_3
\end{align*}
$$

where $\mathrm{Var}[F_1]=1$, $\mathrm{Cov}[\mathbf{\epsilon}, F_1]=\mathbf{0}$ and 

$$
\mathbf{\Psi} = \mathrm{Cov}[\mathbf{\epsilon}] = 
\begin{bmatrix}
0.19 & 0 & 0 \\
0 & 0.51 & 0 \\
0 & 0 & 0.75 
\end{bmatrix}.
$$

That is, write $\mathbf{\rho}$ in the form $\mathbf{L}\mathbf{L}' + \mathbf{\Psi}$, where $\mathbf{L}$ is the matrix of loadings and $\mathbf{\Psi}$ is the diagonal matrix of specific variances.

## Question Two (13)

Use the air pollution data stored in `Air Polution Data.csv`.

a. Generate a sample correlation matrix. (1)
b. Obtain the principal component solution to a factor model with $m=1$ and $m=2$ using the `eigen` function (as before, you must use the `eigen` function). (3)
d. Obtain the maximum likelihood factor model for $m=2$ using `factanal`. (2)
e. Compare the factorization obtained by the principal component and maximum likelihood methods. (2)
f. Perform a varimax rotation of the principal component solution with $m=2$ factors. Interpret the results. (3)
g. Calculate the factor scores from the m=2 maximum likelihood approach using weighted least squares (i.e. using the matrix formula for the factor scores). (2)

## Question Three (4)

In FA we are concerned with accurately capturing the covariance of the variables. 
In this question we prove an upper bound for the sum of squared errors between our fitted covariance matrix and the sample covariance matrix, when we use the principal component method. 

For $\mathbf{S}:p\times p$ the sample covariance matrix, $\hat{\mathbf{L}}:p\times m$ the matrix of loadings estimated using the principal component method and $\hat{\mathbf{\Psi}}$ the diagonal matrix of specific variances, show that 

$$
\mathrm{tr}[(\mathbf{S}-(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}))'(\mathbf{S}-(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}))]\leq \sum_{i=m+1}^p\lambda_i^2.
$$

In other words, show that the sum of squared errors between the sample covariance matrix and the fitted covariance matrix is upper bounded by the sum of the squared eigenvalues of unused principal components.

- Note that $\mathbf{S}-(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}})$ has zeroes on the diagonal, meaning that $\mathrm{tr}[(\mathbf{S}-(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}))'(\mathbf{S}-(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}))]\leq \mathrm{tr}[(\mathbf{S}-\hat{\mathbf{L}}\hat{\mathbf{L}}')'(\mathbf{S}-\hat{\mathbf{L}}\hat{\mathbf{L}}')]$
- Also note that $\mathbf{S}-\hat{\mathbf{L}}\hat{\mathbf{L}}'=\sum_{i=m+1}^p\lambda_i\mathbf{e}_i\mathbf{e}_i'$, where $\mathbf{e}_i$ is the eigenvector of $\mathbf{S}$ associated with the $i$-th largest eigenvalue $\lambda_i$.
- The trace property that $\mathrm{tr}[\mathbf{A}\mathbf{B}\mathbf{C}]=\mathrm{tr}[\mathbf{C}\mathbf{A}\mathbf{B}]=\mathrm{tr}[\mathbf{B}\mathbf{C}\mathbf{A}]$ and the transpose property that $(\mathbf{A}\mathbf{B})'=\mathbf{B}'\mathbf{A}'$ are useful here. 
  
